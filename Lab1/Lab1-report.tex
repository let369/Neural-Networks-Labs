\documentclass{article}
\usepackage[
        a4paper,
        left=3cm,
        right=3cm,
        top=3cm,
        bottom=4cm,
]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[at]{easylist}
\title{Lab Assignement 1}
\date{\today}
\author{
	Karamoulas Eleftherios - S3261859\\
	Tzafos Panagiotis - S3302148\\
}

\begin{document}
\maketitle
\section{Answers for Lab 1 sections 5,6}
\begin{enumerate}[{5.a}]
  \item The error is not decreasing in each epoch because in some iterations with the weight changes the output is the desired one.
  \item Because in the case of goal/output 1/0 and 0/1 even though the error should be the same we have 1 and -1 thats why we use the square.
  \item Because the required iterations to reach 0 depend on the random starting values and the learn rate that we use.
  \item With learning rate 0.6 we observe that more epochs are required to reach 0 error. Using higher learning rate is not always good. The learning rate that should be used depends on the training set.
  \item The TLU is still capable of learning the AND-function after both changes however when we have 0.2 and 0.8 in some cases it needs more epochs to reach 0 error.
  \item In sub-question 5.e we encountered with Resilience to noise our artificial neural network checks if the weighted summation of the inputs is above or below the threshold no matter the difference so small changes in the inputs or the weights don't lead to changes.
  \item TLU can learn the NAND-function however both weights and threshold become negative because the NAND is exactly the opposite of AND so the weights and the threshold are from the other side.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/nand.png}
    \caption{NAND}
    \label{fig:diagram}
  \end{figure}
\end{enumerate}
\begin{enumerate}[{6}]
  \item We observe that the weights, threshold and errors change all the time because the TLU can’t solve nonlinear separable problems and XOR is one of them because of the positions that 1 and 0 have in the coordinate system we can’t a line that separates the outputs in two planes.
  \begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/xor.png}
    \caption{XOR}
    \label{fig:diagram}
  \end{figure}
\end{enumerate}
\section{Code for TLU}
\begin{enumerate}
\item The code starts by initializing the variables like learn rate, nepochs and the tables that are going to be used later like examples, goal, weights, threshold and others that store information about our learning procedure. Then starts the first iteration of how many times our learning algorithm will run and the first values of weight and threshold are stored and then the second iteration for every possible combination of inputs. Then we calculate the weighted summation and after we subtract our threshold from it to find out which will be the output value. Next we check if the output is the desirable and if not we calculate the new weights and threshold. The next step is the computation of the error and the delta values for the weights and threshold. Before plotting the results we update the weights and threshold for the next iteration and store the error values and their mean. Finally we create two plots one for the errors and one for the evolvement of weights and threshold through time.
\lstinputlisting[caption={tlu.m},label={code:bar}]{tlu.m}
\end{enumerate}
\end{document}